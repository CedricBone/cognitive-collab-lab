{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports & Initializations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # Tokenize based on spaces while keeping track of character positions.\n",
    "    tokens = []\n",
    "    char_indices = []\n",
    "    start = 0\n",
    "    for word in text.split():\n",
    "        end = start + len(word)\n",
    "        tokens.append(word)\n",
    "        char_indices.append((start, end))\n",
    "        start = end + 1  # Assuming a single char for space\n",
    "    return tokens, char_indices\n",
    "\n",
    "def align_labels_with_tokens(char_indices, labels_range):\n",
    "    # Initialize labels for each token with 'O' (no label)\n",
    "    labels = ['O'] * len(char_indices)\n",
    "    \n",
    "    for start, end, label in labels_range:\n",
    "        for i, (tok_start, tok_end) in enumerate(char_indices):\n",
    "            if not (end < tok_start or start > tok_end):  # Overlap condition\n",
    "                labels[i] = label\n",
    "    return labels\n",
    "\n",
    "def extract_dataset_labels(json_data):\n",
    "    id_to_label = {0: 'O', 1: 'Organization', 2: 'O', 3: 'Person', 4: 'Person', 5: 'Location', 6: 'Organization', 7: 'O', 8: 'Location'}\n",
    "    \n",
    "    test_labels = {}\n",
    "    labels = {}\n",
    "    label_path = os.path.join( \"data\", \"conll2003\", \"dataset\", \"test.json\")\n",
    "\n",
    "    with open(label_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            # Each line is a separate JSON object\n",
    "            json_line = json.loads(line)\n",
    "            text = \"\"\n",
    "            for word in json_line[\"tokens\"]:\n",
    "                text += word + \" \"\n",
    "            test_labels[text.strip()] = json_line[\"tags\"]\n",
    "\n",
    "    for task in json_data[\"tasks\"]:\n",
    "        text = task[\"data\"][\"text\"].strip()\n",
    "        if text in list(test_labels.keys()):\n",
    "            label = []\n",
    "            for _label in test_labels[text]:\n",
    "                label.append(id_to_label[_label])\n",
    "            labels[text] = label\n",
    "    return labels\n",
    "\n",
    "def extract_model_labels(json_data):\n",
    "    labels = {}\n",
    "    for task in json_data[\"tasks\"]:\n",
    "        text = task[\"data\"][\"text\"]\n",
    "        tokens, char_indices = tokenize(text)\n",
    "        pred_ranges = [(r[\"value\"][\"start\"], r[\"value\"][\"end\"], r[\"value\"][\"labels\"][0]) for p in task[\"predictions\"] for r in p[\"result\"]]\n",
    "        pred_labels = align_labels_with_tokens(char_indices, pred_ranges)\n",
    "        labels[text] = pred_labels\n",
    "    return labels\n",
    "\n",
    "def extract_annotations_labels(json_data):\n",
    "    labels = {}\n",
    "    for task in json_data[\"tasks\"]:\n",
    "        text = task[\"data\"][\"text\"]\n",
    "        tokens, char_indices = tokenize(text)\n",
    "        anno_ranges = [(r[\"value\"][\"start\"], r[\"value\"][\"end\"], r[\"value\"][\"labels\"][0]) for a in task[\"annotations\"] for r in a[\"result\"]]\n",
    "        anno_labels = align_labels_with_tokens(char_indices, anno_ranges)\n",
    "        labels[text] = anno_labels\n",
    "    return labels\n",
    "\n",
    "def extract_inital_predictions_labels(json_data):\n",
    "    labels = {}\n",
    "    for task in json_data[\"tasks\"]:\n",
    "        text = task[\"data\"][\"text\"]\n",
    "        tokens, char_indices = tokenize(text)\n",
    "        pred_ranges = [(r[\"value\"][\"start\"], r[\"value\"][\"end\"], r[\"value\"][\"labels\"][0]) for p in task[\"predictions\"] for r in p[\"result\"]]\n",
    "        pred_labels = align_labels_with_tokens(char_indices, pred_ranges)\n",
    "        labels[text] = pred_labels\n",
    "    return labels\n",
    "\n",
    "def calculate_accuracy(prediction_labels, true_labels, per_sentence=False):\n",
    "    if per_sentence:\n",
    "        for text in prediction_labels:\n",
    "            correct = 0\n",
    "            for pred, true in zip(prediction_labels[text], true_labels[text]):\n",
    "                if pred == true:\n",
    "                    correct += 1\n",
    "            print(f\"Accuracy: {correct/len(prediction_labels[text])}\")\n",
    "    else:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for text in prediction_labels:\n",
    "            for pred, true in zip(prediction_labels[text], true_labels[text]):\n",
    "                if pred == true:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "        print(f\"Accuracy: {correct/total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_type = \"solo\"\n",
    "annotation_path = os.path.join(\"data\", f\"{trial_type}\", \"annotations\")\n",
    "annotations = {}\n",
    "inital_model_predictions = {}\n",
    "\n",
    "for file in os.listdir(annotation_path):\n",
    "    if file.endswith(\"_annotations.json\"):\n",
    "        with open(os.path.join(annotation_path, file), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            participant_id = str(file.split(\"_\")[1])\n",
    "            annotations[participant_id] = data\n",
    "    elif file.startswith(\"model_predictions\"):\n",
    "        with open(os.path.join(annotation_path, file), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            inital_model_predictions = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'SOCCER - JAPAN GET LUCKY WIN , CHINA IN SURPRISE DEFEAT .': ['Person', 'O', 'Person', 'O', 'O', 'O', 'O', 'Person', 'O', 'O', 'O', 'O'], 'Japan began the defence of their Asian Cup title with a lucky 2-1 win against Syria in a Group C championship match on Friday .': ['Organization', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Organization', 'O', 'O', 'Organization', 'O', 'O', 'O', 'O', 'O', 'O'], 'But China saw their luck desert them in the second match of the group , crashing to a surprise 2-0 defeat to newcomers Uzbekistan .': ['O', 'Organization', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'China controlled most of the match and saw several chances missed until the 78th minute when Uzbek striker Igor Shkvyrin took advantage of a misdirected defensive header to lob the ball over the advancing Chinese keeper and into an empty net .': ['Organization', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Organization', 'O', 'Location', 'Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'Oleg Shatskiku made sure of the win in injury time , hitting an unstoppable left foot shot from just outside the area .': ['Location', 'Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'The former Soviet republic was playing in an Asian Cup finals tie for the first time .': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'Despite winning the Asian Games title two years ago , Uzbekistan are in the finals as outsiders .': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'Two goals from defensive errors in the last six minutes allowed Japan to come from behind and collect all three points from their opening meeting against Syria .': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Organization', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Organization', 'O'], 'Takuya Takagi scored the winner in the 88th minute , rising to head a Hiroshige Yanagimoto cross towards the Syrian goal which goalkeeper Salem Bitar appeared to have covered but then allowed to slip into the net .': ['Person', 'Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Person', 'Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Person', 'Organization', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'It was the second costly blunder by Syria in four minutes .': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'Organization', 'O', 'O', 'O', 'O'], \"Defender Hassan Abbas rose to intercept a long ball into the area in the 84th minute but only managed to divert it into the top corner of Bitar 's goal .\": ['Person', 'Person', 'Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Organization', 'O', 'O', 'O'], 'Nader Jokhadar had given Syria the lead with a well-struck header in the seventh minute .': ['Location', 'Location', 'O', 'O', 'Organization', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'Japan then laid siege to the Syrian penalty area for most of the game but rarely breached the Syrian defence .': ['Organization', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'Bitar pulled off fine saves whenever they did .': ['Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'Hosts UAE play Kuwait and South Korea take on Indonesia on Saturday in Group A matches .': ['O', 'Organization', 'O', 'Organization', 'O', 'Organization', 'Organization', 'O', 'O', 'Organization', 'O', 'O', 'O', 'Organization', 'O', 'O', 'O'], 'All four teams are level with one point each from one game .': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'RUGBY UNION - CUTTITTA BACK FOR ITALY AFTER A YEAR .': ['Person', 'O', 'O', 'Location', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'Italy recalled Marcello Cuttitta': ['Organization', 'Person', 'Person', 'O'], 'on Friday for their friendly against Scotland at Murrayfield more than a year after the 30-year-old wing announced he was retiring following differences over selection .': ['O', 'O', 'O', 'O', 'O', 'O', 'Organization', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'Cuttitta , who trainer George Coste said was certain to play on Saturday week , was named in a 21-man squad lacking only two of the team beaten 54-21 by England at Twickenham last month .': ['O', 'O', 'O', 'O', 'Person', 'Person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Organization', 'O', 'Organization', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "source": [
    "inital_predictions_labels = extract_inital_predictions_labels(inital_model_predictions)\n",
    "print(inital_predictions_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial model accuracy with respect to dataset:\n",
      "Accuracy: 0.9009433962264151\n",
      "\n",
      "Participant 001\n",
      "Annotation accuracy with respect to dataset:\n",
      "Accuracy: 0.9528301886792453\n",
      "Updated model accuracy with respect to dataset:\n",
      "Accuracy: 0.9009433962264151\n",
      "Updated model accuracy with respect to annotation:\n",
      "Accuracy: 0.8773584905660378\n",
      "Initial model accuracy with respect to annotation:\n",
      "Accuracy: 0.8773584905660378\n",
      "\n",
      "Participant 002\n",
      "Annotation accuracy with respect to dataset:\n",
      "Accuracy: 0.9457547169811321\n",
      "Updated model accuracy with respect to dataset:\n",
      "Accuracy: 0.9009433962264151\n",
      "Updated model accuracy with respect to annotation:\n",
      "Accuracy: 0.8679245283018868\n",
      "Initial model accuracy with respect to annotation:\n",
      "Accuracy: 0.8679245283018868\n",
      "\n",
      "Participant 004\n",
      "Annotation accuracy with respect to dataset:\n",
      "Accuracy: 0.9221698113207547\n",
      "Updated model accuracy with respect to dataset:\n",
      "Accuracy: 0.9009433962264151\n",
      "Updated model accuracy with respect to annotation:\n",
      "Accuracy: 0.8867924528301887\n",
      "Initial model accuracy with respect to annotation:\n",
      "Accuracy: 0.8867924528301887\n",
      "\n",
      "Participant 003\n",
      "Annotation accuracy with respect to dataset:\n",
      "Accuracy: 0.9504716981132075\n",
      "Updated model accuracy with respect to dataset:\n",
      "Accuracy: 0.9009433962264151\n",
      "Updated model accuracy with respect to annotation:\n",
      "Accuracy: 0.8844339622641509\n",
      "Initial model accuracy with respect to annotation:\n",
      "Accuracy: 0.8844339622641509\n",
      "\n",
      "Participant 005\n",
      "Annotation accuracy with respect to dataset:\n",
      "Accuracy: 0.8584905660377359\n",
      "Updated model accuracy with respect to dataset:\n",
      "Accuracy: 0.9009433962264151\n",
      "Updated model accuracy with respect to annotation:\n",
      "Accuracy: 0.8632075471698113\n",
      "Initial model accuracy with respect to annotation:\n",
      "Accuracy: 0.8632075471698113\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_labels = extract_dataset_labels(inital_model_predictions)\n",
    "inital_model_labels = extract_model_labels(inital_model_predictions)\n",
    "\n",
    "print(\"Initial model accuracy with respect to dataset:\")\n",
    "calculate_accuracy(inital_model_labels, dataset_labels, per_sentence=False)\n",
    "\n",
    "print()\n",
    "\n",
    "for participant in annotations.keys():\n",
    "    print(f\"Participant {participant}\")\n",
    "    updated_model_labels = extract_model_labels(annotations[participant])\n",
    "    annotation_labels = extract_annotations_labels(annotations[participant])\n",
    "    \n",
    "    print(\"Annotation accuracy with respect to dataset:\")\n",
    "    calculate_accuracy(annotation_labels, dataset_labels, per_sentence=False)\n",
    "\n",
    "    print(\"Updated model accuracy with respect to dataset:\")\n",
    "    calculate_accuracy(updated_model_labels, dataset_labels, per_sentence=False)\n",
    "\n",
    "    print(\"Updated model accuracy with respect to annotation:\")\n",
    "    calculate_accuracy(updated_model_labels, annotation_labels, per_sentence=False)\n",
    "\n",
    "    print(\"Initial model accuracy with respect to annotation:\")\n",
    "    calculate_accuracy(inital_predictions_labels, annotation_labels, per_sentence=False)\n",
    "\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
